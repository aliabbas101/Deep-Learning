{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras                    \n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb=keras.datasets.imdb\n",
    "(train_data,train_labels),(test_data, test_labels)=imdb.load_data(num_words=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=imdb.get_word_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\". format(len(train_data),len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={k:(v+3) for k,v in word_index.items()}\n",
    "#Reserved Words\n",
    "word_index['<PAD>']=0\n",
    "word_index['<START>']=1\n",
    "word_index['<UNK>']=2\n",
    "word_index['<UNUSED>']=3\n",
    "\n",
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=keras.preprocessing.sequence.pad_sequences(train_data, \n",
    "                                                      value=word_index[\"<PAD>\"], \n",
    "                                                      padding='post', maxlen=256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=10000\n",
    "\n",
    "model=keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1,activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val= train_labels[:10000]\n",
    "partial_y_train=train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 4s 265us/sample - loss: 0.6914 - acc: 0.5193 - val_loss: 0.6890 - val_acc: 0.5638\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.6840 - acc: 0.6257 - val_loss: 0.6795 - val_acc: 0.6415\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.6696 - acc: 0.6949 - val_loss: 0.6626 - val_acc: 0.7146\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.6463 - acc: 0.7369 - val_loss: 0.6364 - val_acc: 0.7648\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.6125 - acc: 0.7867 - val_loss: 0.6013 - val_acc: 0.7889\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.5699 - acc: 0.8179 - val_loss: 0.5606 - val_acc: 0.8029\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 68us/sample - loss: 0.5223 - acc: 0.8331 - val_loss: 0.5159 - val_acc: 0.8251\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 0.4748 - acc: 0.8509 - val_loss: 0.4749 - val_acc: 0.8365\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.4306 - acc: 0.8638 - val_loss: 0.4381 - val_acc: 0.8456\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.3918 - acc: 0.8765 - val_loss: 0.4079 - val_acc: 0.8534\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.3591 - acc: 0.8831 - val_loss: 0.3831 - val_acc: 0.8601\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.3318 - acc: 0.8917 - val_loss: 0.3643 - val_acc: 0.8630\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.3094 - acc: 0.8970 - val_loss: 0.3476 - val_acc: 0.8688\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.2893 - acc: 0.9017 - val_loss: 0.3355 - val_acc: 0.8731\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.2725 - acc: 0.9071 - val_loss: 0.3253 - val_acc: 0.8746\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.2575 - acc: 0.9114 - val_loss: 0.3171 - val_acc: 0.8756\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.2437 - acc: 0.9170 - val_loss: 0.3102 - val_acc: 0.8793\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 75us/sample - loss: 0.2314 - acc: 0.9211 - val_loss: 0.3042 - val_acc: 0.8810\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.2203 - acc: 0.9231 - val_loss: 0.2993 - val_acc: 0.8818\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 68us/sample - loss: 0.2103 - acc: 0.9279 - val_loss: 0.2960 - val_acc: 0.8822\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.2001 - acc: 0.9323 - val_loss: 0.2931 - val_acc: 0.8830\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.1916 - acc: 0.9354 - val_loss: 0.2904 - val_acc: 0.8845\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.1830 - acc: 0.9395 - val_loss: 0.2895 - val_acc: 0.8834\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.1755 - acc: 0.9430 - val_loss: 0.2882 - val_acc: 0.8844\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 68us/sample - loss: 0.1678 - acc: 0.9462 - val_loss: 0.2866 - val_acc: 0.8856\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.1611 - acc: 0.9483 - val_loss: 0.2873 - val_acc: 0.8836\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.1546 - acc: 0.9515 - val_loss: 0.2867 - val_acc: 0.8855\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.1484 - acc: 0.9547 - val_loss: 0.2874 - val_acc: 0.8848\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 77us/sample - loss: 0.1429 - acc: 0.9571 - val_loss: 0.2887 - val_acc: 0.8844\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.1372 - acc: 0.9588 - val_loss: 0.2884 - val_acc: 0.8860\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.1314 - acc: 0.9610 - val_loss: 0.2895 - val_acc: 0.8861\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.1263 - acc: 0.9641 - val_loss: 0.2910 - val_acc: 0.8855\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.1212 - acc: 0.9655 - val_loss: 0.2933 - val_acc: 0.8849\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.1167 - acc: 0.9667 - val_loss: 0.2956 - val_acc: 0.8855\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.1126 - acc: 0.9679 - val_loss: 0.2981 - val_acc: 0.8852\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.1081 - acc: 0.9699 - val_loss: 0.2999 - val_acc: 0.8843\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.1037 - acc: 0.9713 - val_loss: 0.3027 - val_acc: 0.8840\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0998 - acc: 0.9728 - val_loss: 0.3064 - val_acc: 0.8823\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 77us/sample - loss: 0.0966 - acc: 0.9741 - val_loss: 0.3101 - val_acc: 0.8820\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.0924 - acc: 0.9758 - val_loss: 0.3130 - val_acc: 0.8831\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(partial_x_train,\n",
    "                 partial_y_train,\n",
    "                 epochs=40,\n",
    "                 batch_size=512,\n",
    "                 validation_data=(x_val,y_val),\n",
    "                 verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 43us/sample - loss: 0.3343 - acc: 0.8707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3342530643081665, 0.87068]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_data)\n",
    "rand_review=np.random.randint(10000, size=256)\n",
    "biased_review=np.full(256,531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08120653],\n",
       "       [0.99909514],\n",
       "       [0.6567853 ],\n",
       "       ...,\n",
       "       [1.        ],\n",
       "       [0.34709793],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=np.append(test_data,[rand_review],axis=0)\n",
    "test_data=np.append(test_data,[biased_review],axis=0)\n",
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brooding wherever lame jason smoothly notably sense mockery robbed comedians claimed continued hopeful reno sort insightful birthday recreation failure comparison plotted millions string weirdo remainder teenager incredibly scared directions nerdy proof prejudices intrigue guard tyler studying garden thief voice peers compared othello training trains centers at banter national mentioned outright dwight woke documentaries masterpiece payne novels arrogance hostel rome we'll comics hats kansas spoofs destination splendid marcel deborah intentions annoyance kings yesterday dylan residents shelf known transferred lifts fodder cycle switching gothic poster remove seed principal gilliam positions remind greatly tide commentary martial legs poke if misfire saves really common somewhat hardly hat rko stretches combining telly wild lesbian quinn german rant claire imaginary beauty noted bullying greatest realised gere europeans lyrical duvall others horrible sullivan photographed prom remark mutated interesting villain always police slaves des sheets fbi another magazine depardieu applied tries course edison ace hammy dutch fury creation couple thus help 2003 dialogues dracula kornbluth keitel cyborg deeper direct souls lends arrow eric henchman mainly tricked formulaic thumb mother's kicks character inexplicable altman anticipated hopefully sleepy unborn thrown confusing 1964 don't dealing zone wars disgusting respectively kung standpoint garde helpful fundamental energetic vividly truthful ish about arts jones' elephants cortez theaters seedy heartfelt brenda norma earned album silent psychiatrist joe crossing carrie enter tend 1969 respect purposes emma worrying 1951 distraction beats che's wooden sober gory favourites needing she'd wallace until appropriately ambiance men's smart manipulative deal hate carpet symbolism nostalgic reno bathroom straightforward symbolic uneasy convincingly sales denver regain swinging origins resembles bullying\n",
      "<START> a good ol' boy film is almost required to have <UNK> car chases a storyline that has a vague resemblance to plot and at least one very pretty country gal <UNK> with short shorts and a low top the pretty gal is here dressed in designer <UNK> but the redneck <UNK> stop there jimmy dean is a natural as a <UNK> <UNK> but as a tough guy former sheriff he comes up way short big john is big but he isn't convincing with the bad part of his <UNK> bug eyed jack <UNK> is a hoot as always and bo hopkins has been playing this same part for decades ned beatty also does his part in a small role but there is no story it <UNK> more like an episode of in the heat of the night than a feature film <UNK> with easily predictable sentiment perhaps the most glaring problem with this movie is charlie daniels singing the theme you know the one it was made famous by jimmy dean <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(rand_review))\n",
    "print(decode_review(test_data[-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-c9bbde865eaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
